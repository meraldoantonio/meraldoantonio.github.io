---
title: "Interview with Apache Pig: Theory and Practical Guide"
date: 2019-10-01
category: Mini-exercise
tags: [big data]
excerpt: "In this exclusive interview, Apache Pig spoke about his role in the Hadoop ecosystem and elaborated on his use cases and his differences from other Hadoop components such as Hive and Spark. He also provided some practical guidance on how to start using him. Let‚Äôs hear what Pig has to say!"
mathjax: "true"
---

*Apache Pig is an integral component of the Hadoop ecosystem. Since his birth more than a decade ago, he has helped thousands of analysts around the world make sense of massive data sets. Despite his importance, few people outside of the big data sector have heard of him, and some who are acquainted with him have only vague idea about his usage. A few days ago, I had the pleasure to sit down and talk with Apache Pig himself. Our discussion covered many topics ‚Äî from his founding philosophy to practical guidance on writing his language, Pig Latin. Below is the excerpt of this interview. You can find a copy of this article in [Towards Data Science](https://towardsdatascience.com/interview-with-apache-pig-theory-and-practical-guide-6c44f06a4ca9).*

---

# Hi Pig! Thank you for this interview opportunity!

Thank you! It‚Äôs my pleasure to give this interview and gain some additional exposure for myself.
You know, I‚Äôm tired of people confusing me with my buddies in the Hadoop ecosystem. To many, we are just a mishmash of colorfully-named tools that somehow fit together to facilitate the management of Big Data. This has got to change! We play distinct roles in the Hadoop ecosystem and a familiarity with each of us is key to understanding Big Data. It‚Äôs about time that I cleared this confusion and told the world who I really am!

<p style="text-align: center;">
<img src="https://miro.medium.com/max/452/0*KauzwM211rZFxp5G" alt="Apache Pig">
</p>
<p style="text-align: center;">
<i>C'est moi - Apache Pig!</i>
</p>

# Great, that‚Äôs exactly what I‚Äôm here for! Now for the sake of our casual readers who are just getting started to the world of Big Data, could you please introduce yourself?

Of course! My name is Apache Pig, but most people just call me Pig. I am an open-source tool for analyzing large data sets.
I was conceived by Yahoo in 2006. My conception was driven by Yahoo‚Äôs need to transform and analyze massive amount of data without having to write complex MapReduce programs. In 2007, Yahoo open-sourced me under Apache Software Foundation (hence my first name). Since then, I have been guzzling and analyzing petabytes worth of data all around the world.

# I like your name! Can you elaborate on its origin?

Thanks! It is an interesting name, isn‚Äôt it? Many people think that my name stems from my creators‚Äô effort to stick to the animal theme that permeates the Hadoop world. This is partly true ‚Äî my name was indeed inspired by the eponymous animalüê∑. This name, in turn, led to the adoption of cute nomenclatures for concepts related to me such as *Pig Latin* ‚Å†‚Äîmy programming language‚Äî and *Grunt‚Å†* ‚Äîmy interactive shell (more on these later).

<p style="text-align: center;">
<img src="https://miro.medium.com/max/2930/0*aXJz9_1KyMUXoMg3" alt="Apache Pig">
</p>
<p style="text-align: center;">
<i>Apache Pig‚Äôs similarity to this cute fella is more than skin deep!</i>
</p>

However, apart from it being catchy, there is actually an interesting philosophy that underlies my name. This philosophy, which is somewhat related to the characteristics of pig the animal, consists of four principles that guide how I should be developed:
1. **Pigs eat anything.** I am a voracious data-guzzler. I can process any type of data, be it relational, flat, nested, structured or unstructured. I‚Äôm also capable of reading data from multiple sources, such as Hive, HBase and Cassandra.
2. **Pigs live anywhere.** Although I was first implemented on Hadoop, I‚Äôm not intended to be tied to this framework. For instance, I can be run in a single computer to analyze local files.
3. **Pigs are domestic animals.** I was designed to be easily understandable and modifiable by users. My programming language, Pig Latin, is easy to wrap one‚Äôs head around. In addition, its capabilities can be easily extended by importing various user defined functions (UDFs).
4. **Pigs fly.** This motto speaks to the speed with which I can process data. It also embodies my ability to help my users express complex logic in just a few lines of Pig Latin code.

# That‚Äôs very interesting. Now, you mentioned that you are part of the Hadoop ecosystem. Can you tell us a bit about how you fit in this ecosystem?

Certainly! To start off, for the sake of those who are new to this field, let me give you a quick explanation about Hadoop.

>  Hadoop is a collection of open-source tools with which we can harness the combined power of several computers to store and process massive amounts of data.

The diagram below depicts how I relate to other components within the ecosystem.

<p style="text-align: center;">
<img src="https://miro.medium.com/max/2142/1*0eSXK6JvX3A0z22FUPZpDg.png" alt="Apache Pig Diagram">
</p>
<p style="text-align: center;">
<i>Pig and other Hadoop components</i>
</p>

The core of Hadoop is **HDFS (Hadoop Distributed File System)** and **YARN (Yet Another Resource Negotiator).** I work closely with these two guys.

HDFS is a distributed storage system. It breaks files into blocks, makes redundant copies of these blocks and distributes them across different machines in the cluster. **You can think of HDFS as my data manager.** I read input files from HDFS, temporarily store my intermediate calculation results in HDFS and write the output of my analysis to HDFS.

On the other hand, **you can think of YARN as my computing resource manager.** YARN negotiates computing resources on my behalf and schedules the tasks I need to execute. In doing so, YARN works closely with HDFS. YARN makes sure that the computing resources I get are located close (in terms of network topology) to the machines that store the data that I have to analyze.

# In the diagram above, I see that there is an execution engine layer between YARN and yourself. Could you explain what an execution engine is?

Good question! YARN is actually as my *indirect* computing resource manager ‚Äî I don‚Äôt interface directly with him. Instead, this interaction takes place through an execution engine layer. In Hadoop‚Äôs context, an *execution engine* is a software system or a framework that runs across a cluster and gives the illusion of the cluster being a single giant machine. Most execution engines run on top of YARN. **As of today, there are three execution engines I can run on: [MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html), [Tez](https://tez.apache.org/) and [Spark](https://spark.apache.org/).**

<p style="text-align: center;">
<img src="https://miro.medium.com/max/1453/1*D3U2w0zAe6qwKfcEsolb8g.png" alt="Apache Pig Execution Engines">
</p>
<p style="text-align: center;">
<i>The three execution engines that Pig can run on: Hadoop, Tez and Spark.</i>
</p>

# You‚Äôve previously singled out MapReduce and said that you obviate the need for writing MapReduce code. Can you elaborate more on what MapReduce is?

Sure! As I mentioned earlier, MapReduce is one of the three execution engines that I can run on. It is not a piece of software per se; rather, it is a simple yet powerful *programming paradigm* (i.e. a way of designing a program). **Every MapReduce program consists of three phases: map, shuffle, and reduce.**

I shall first define these terms conceptually. Suppose we have a giant file on which we‚Äôd like a MapReduce program to operate. **In the map phase, the program will look at this giant file line by line.** For each line, the program will output a key-value pair. The exact identity of these keys and values will depend on the objective of the program.

**In the shuffle phase, the key-value pairs with the same keys are grouped together.** The resulting groups will be ordered according to their keys (e.g. if the keys are strings, then the groups will be sorted by the alphabetical structure of the keys). The output of the shuffle phase is an ordered bunch of groups. These groups are then distributed to different machines for the subsequent reduce phase.

**In the reduce phase, an aggregation function is applied to the bundled values in each group produced in the shuffle phase.** This process is performed in parallel in many machines. The reduce phase will output the result of the program.

An example will make this clearer. Let‚Äôs say I have a text file and I want to count the occurrence of each distinct word in the text. Here is how a MapReduce job for this task would look like:

<p style="text-align: center;">
<img src="https://miro.medium.com/max/2739/1*6AFn9DCVIi0ydCZFoAUGeg.png" alt="MapReduce for word count">
</p>
<p style="text-align: center;">
<i>MapReduce for word count</i>
</p>

The above example looks simple, doesn‚Äôt it? Unfortunately, this is not the case for a great majority of MapReduce programs! Any big data professional can tell you that writing MapReduce programs is cumbersome and that not all data procedures can be easily written using the MapReduce paradigm.
This is where I come in! It turns out that my language, Pig Latin, is much easier to write than MapReduce (one estimate points out that 10 lines of Pig Latin are equivalent to 200 lines of MapReduce code in Java!). When I run on MapReduce as the execution engine, under the hood, all these Pig Latin codes are converted into their MapReduce programs. Pretty neat, huh?
